[Getting Pixels onto the Screen](https://www.objc.io/issues/3-views/moving-pixels-onto-the-screen/)



​	一个像素是如何绘制到屏幕上的呢？有很多方法可以将某些东西绘制到显示器上，他们涉及到很多不同的框架以及函数与方法的结合体。这里我们可以探究一些在屏幕后面发生的事情。我们希望这可以帮助你理解，当你需要决定什么时候并且怎么样去调试喝修复性能问题的时候，哪个API可以最好的去工作。我们将会专注讨论iOS，当然，大部分的讨论结果，同样适用于OS X。

### 图形栈

当像素绘制到屏幕上时，后台其实做了非常多的事情。一旦他们被渲染到屏幕上时，每个像素都是由红，绿，蓝三种颜色构成的。三个被特定强度点亮的颜色单元，给我们的印象就是一个特定颜色的像素（其实就是三个颜色单元 + alpha = 像素）。在你的iPhone5上，液晶屏有1136 * 640 = 727070个像素，因此有2181120个颜色单元。在15寸带视网膜屏幕的的MacBook Pro中，就拥有15.5亿个像素。整个图形栈协同工作以确保图形被正确的显示。当你全屏滑动时，数以百万计的像素必须以每秒60次的频率刷新。这是一个很大的工作量。

### 软件组成

从一个简单的角度看，一个软件的堆栈就像下面这样：

![](https://www.objc.io/images/issue-3/pixels-software-stack@2x_1ae69f5.png)

在显示器的上一层就是GPU，即图形处理单元。GPU是一个高效的并发处理单元，并且他是为图形的并行计算而特别量身定制的。这也是为什么它能够更新所有的像素，并且将结果绘制到屏幕上去。它并行的特性也就允许它能高效的将不同的纹理合并成一个。我们将会说更多与合成细节相关的东西。关键点就是，GPU是如此的特俗，因此它可以非常高效的做一些特定的工作。相比于CPU来说，GPU会花费更少的时间，更快的速度去完成这些工作。普通的CPU有一个常用的目的：它可以处理很多不同的事物，但是对于合成图像，它会执行的非常慢。

GPU的驱动是一系列直接与GPU交互的代码。不同的GPU是不同的野兽，但是驱动使他们在下一层级显示的更为统一，这里的下一层级就是OpenGL/OpenGL ES。

OpenGL（Open Graphic Library）是一个为了渲染2D和3D图像的API。因为GPU是一块非常特殊的硬件，OpenGL与GPU工作非常紧密，以促进GPU的能力以及实现硬件加速的渲染。对于很多人来说，OpenGL可能看起来非常底层，但是当他第一次在1992年发布的时候，它是第一个主要与图形硬件交流的标准化的方法，这是一个重大的飞跃，因为程序员不需要再为每个GPU重写他们的App。

OpenGL的上一层被分为了几块。在iOS上，几乎所有的东西都是通过Core Animation绘制出来的，但是在OS X上，绕过Core Animation去用Core Graphic的情况并不少见。对于一些特殊的应用，特别是游戏，App可能会与OpenCL/OpenGL ES直接交流。然后事情变得越来越扑所迷离，因为在某些东西的渲染上，Core Animation会使用到Core Graphic。一些像AVFoundation，Core Image 以及其他一些混合入口的框架。

有一件事情我们必须要知道，GPU是一个非常强大的图形处理硬件，并且在展示像素的时候起着核心的作用。它连接到了CPU。从硬件上来说，它们之间存在着某种总线。这里有一些像OpenGL，Core Animation，以及Core Graphic的框架编排着GPU与CPU之间传递的数据。为了让你的像素展示到屏幕上，一些处理工作会在CPU上完成。然后数据会传输到GPU相对应的也会对数据做一些处理，然后最后你的像素就会显示到屏幕上。

这个过程中的每一部分都有自己的挑战，并且许多时候需要做出一些折中的选择。

### 硬件参与者

![](https://www.objc.io/images/issue-3/pixels%2C%20hardware%402x.png)

正如上面这张简单的图片所带来的挑战这样：GPU具有为每一帧（一秒钟60次）所合成的纹理（位图）。每个纹理都占用着VRAM（Virtual random access memory：虚拟随机访问内存）， 因此它也限制了GPU可以拥有的纹理数量。GPU在合成上是非常高效的，但是一些特定的合成任务会比其他的更加的复杂，并且GPU在16.7ms中能做的事情是有限的。

接下来的挑战就是把你的数据传递到GPU。为了能让GPU接收数据，它需要从RAM被传递到VRAM。这个过程可以看成是被上传到GPU。这可能看起来有点微不足道，但是对于很大的纹理来说是非常耗时的。

最后，CPU会运行你的程序。你可能会告诉CPU从bundle中加载一张PNG格式的图片，并且解码它。所有的这些都发生在CPU。当你想要展示这张解码的图片时，它就需要通过某种方式上传到GPU。像一些展示文本这种及其简单的事，对于CPU来说也是非常复杂的。这会促使Core Text与Core Graphics更加紧密的合作来生成一张文本的位图。一旦准备就绪，它会作为纹理被上传到GPU，然后准备被渲染到屏幕上去。当你滑动或者在屏幕上移动文本时，一些非常相似的纹理会被重用，并且CPU会简单的告诉GPU一些新的位置坐标，所以GPU可以重用已经存在的纹理。CPU不需要去重新渲染文本，位图也不需要被重新上传。

上述这些说明了所涉及到的复杂性。有个大概的了解之后，我们再深入了解所涉及到的技术。



### 合成

合成在图像世界中，是一个描述怎么把不同的位图合并到一起，最终形成一张图片展示到屏幕上的术语。这在很多情况下，显然会让我们很轻易的遗忘它所涉及到的计算的复杂性。

让我们先忽略掉一些深奥的例子，然后假设所有在屏幕上展示的都是纹理。纹理是一个存储着RGBA数据的矩形区域。每个像素都包含了红，绿，蓝以及透明度的值。在Core Animation中，这也是CALayer的基础组成部分。

在这个简化的设置中，每个layer都是一个纹理，所有的这些纹理都以某种方式堆叠在彼此的顶端。对于每个在屏幕上的像素来说，GPU需要算出如何混合这些纹理中的像素后的RGBA的值。这也是合成的意思。

如果所有我们拥有的都是一张屏幕大小的简易的纹理，并且与屏幕像素点对齐，每个在屏幕上的像素都会对应纹理中的像素。那么纹理的像素也就是屏幕的像素。

如果我们有另一张纹理被放置在了第一张纹理上，GPU就会需要在第一张纹理上去合成这张纹理。这里有不同的混合模式，但是如果我们假设两张纹理都是像素对齐的，我们也使用普通的混合方式，那么结果的颜色就可以根据下面这个公式获得：

``` R = S + D * (1 - Sa)
R = S + D * (1 - Sa)
```

结果的颜色就是源颜色（即在上面一层的纹理的像素值）加上目标颜色（第一层级的像素值）乘以 1 减去源颜色的透明度。所有在这个公式中的颜色，我们都假定已经预先乘了它们的透明度。

很显然这里会发生一些事情。让我们做第二个假设，那就是所有的纹理都是不透明的，即alpha=1。如果目标纹理是蓝色，源纹理是红色，因为源纹理的透明度是1，因此结果就是：

```
R = S
```

那么结果就是源纹理的红色。这也是你所期望看到的。

如果源色层是50%的透明度，即alpha=0.5，因为RGB的预先乘了Alpha的值，那么S的RGB的值就是（0.5， 0， 0）。那么合成的计算公式就会像下面这样：

```
								
                       0.5   0               0.5
R = S + D * (1 - Sa) = 0   + 0 * (1 - 0.5) = 0
                       0     1               0.5
```

我们最终会得到的RGB值为（0.5， 0， 0.5），它可能是饱和的李子的颜色或者紫色。当在蓝色背景上混合半透明的红色时这就是我们直觉所期待看到的。

记住我们只是将一个纹理的像素与另一个纹理的像素进行了合成。GPU需要在两个纹理重合的时候对所有的像素执行合成的操作。就像你所知道的，大部分的app都有很多的层级，因此这些纹理需要被一起合成。这将会使GPU非常的忙碌，即使这是能够非常高效执行这些事情的硬件。

### 不透明 vs 透明

当源纹理绝对不透明的时候，结果的纹理就是源纹理。这可以节省很多GPU的工作量，它可以简单的从源纹理复制，而不是合成所有的像素。但是GPU是没有办法知道纹理中的像素是透明的还是不是透明的。只有程序员能知道你在视图层上到底放了什么。这也是为什么CALayer有个属性叫做opaque。如果这个值为YES，那么GPU就不会做任何合成的操作，只是简单的从视图进行复制，而不用去管这个事图下面还有什么。这为GPU节省了非常多的工作。这也是Instrument工具中color blended layers选项所涉及的。它可以允许你看到那个图层（纹理）被标记为不透明的，那些图层让GPU做了合成的操作。合成不透明的图层是非常廉价的，因为涉及到了更少的数学操作。

如果你知道你的图层是不透明的，那就设置opaque属性为YES。如果你加载一张没有alpha通道的图片，并且在UIImageView中展示，那么这将自动发生。但是需要注意的是，一张图片没有alpha通道，跟有100%alpha值是完全不一样的。对于后一种情况，Core Animation会假设这张图片可能alpha值不是100%。在Finder中，你可以使用Get Info去获取到更多信息。它会告诉你，这张图片有没有alpha通道。

### 像素对齐与不对齐

到目前为止，我们看到的图层的像素在展示的时候都是完美对齐的。当所有的东西都是像素对齐的时候，我们可以获得相对简单的数学运算。每当GPU需要计算出屏幕上的像素颜色的时候，它只需要关注这个屏幕上方的图层的像素，并将他们组合到一起。或者，如果最上层的纹理是不透明的，那么GPU只需要简单的从这个纹理进行像素的拷贝。

当一个图层上的像素与屏幕上的像素完美对齐的时候，那么我们就说这个图层是像素对齐的。这里有两个主要的原因会造成像素的不对齐。第一个原因就是缩放，当一个纹理被放大或者缩小的时候，纹理的像素就将不会跟屏幕对齐。第二个原因就是纹理的起点不跟像素边界对齐。（比如origin = {10.2, 50.3 }）

在这两种情况下，GPU都需要做额外的数学运算。它会将多个像素与源像素进行混合生成的新值进行合成。当所有的图层都是像素对齐的时候，GPU就只需要做很少的工作。

再一次的，Core Animation Instrument有一个叫做color misaligned images的选项，会告诉我们什么时候会在图层上发生像素不对齐的情况。

### 蒙板

一个图层可以有一个与之关联的蒙版。蒙板是一个拥有透明度的位图，在图层与下面的内容进行合成之前，蒙板会应用到这个图层上去。当你要设置一个圆角，你可以有效的在图层上设置一个蒙板。当然，它也可以指定任意的蒙板，比如字母A。只有在蒙板中的图层才会被渲染出来。

### 离屏渲染

离屏渲染可以被Core Animation自动触发，或者被应用强制触发。屏幕外的渲染/渲染图层会合并图形树的一部分到一个新的缓冲区（这是离屏，不是在屏幕上），然后这个缓冲区将被渲染到屏幕上。

当合成计算成本很高的时候，你可能想要强制执行离屏渲染。它可以缓存合成的纹理/图层。如果你的渲染树（所有的纹理以及如何组合到一起）非常复杂，你可以强制执行离屏渲染去缓存这些图层，然后使用这些缓存合成到屏幕上。

如果你的应用组合了一些图层，然后你想要让它们一起运动。对于每一帧，GPU会通常会重新渲染所有这些图层。当使用离屏渲染的时候，GPU首先会组合这些图层到一个基于新纹理的位图缓存中去，然后将这个新的纹理绘制到屏幕上。现在当那些图层被移动到了一起，GPU可以重新使用这个位图缓存，然后可以做更少的工作。值得注意的是，这个只在这些图层不会改变的前提下工作。如果它们确实改变了，那么GPU必须重新创建一个位图缓存。当你设置shouldRasterize属性等于YES的时候，可以触发这个功能。

然而这项功能是需要权衡的。一方面，这可能会让事情变得更慢。创建这个额外的离屏缓存，GPU必须需要执行一个新增的步骤，并且如果GPU不能重复使用这个位图，那么将浪费性能。即使位图被重用了，GPU也可能将它卸载。你必须去测算GPU的利用率以及帧率去判断它是否真的有用。

离屏渲染也会产生一些副作用。如果你直接或者间接的将一个蒙板应用到图层上，Core Animation将会强制执行离屏渲染去应用这个蒙板。这会对GPU产生一定的负担。通常GPU只能直接渲染到帧缓存区（即屏幕上）。

Instruments的Core Animation工具有选项叫做Color Offscreen-Rendered Yellow，它会将已经用离屏缓冲区渲染的区域标记为黄色（这个选项同样也适合模拟器的调试菜单）。确保你也勾选了Color Hits Green and Misses Red。绿色表示当离屏缓冲区被重用了，红色则表示需要重新创建新的离屏缓冲区。

通常情况下，你会想要去避免离屏渲染，因为它非常昂贵。直接合成图层到帧缓冲区比第一次创建离屏缓冲区要廉价很多，离屏缓冲区要先渲染这部分离屏的图层，然后再将结果返回到帧缓冲区。这样就涉及到了两个昂贵的上下文切换操作（将上下文切换到离屏缓冲区，再切回到帧缓冲区）。

当你打开Color Offscreen-Rendered Yellow的时候，你看到了黄色的标记，那么你就需要注意了。但是这可能并不是很坏。如果Core Animation能够重用这部分离屏渲染的结果的话，它可能还能通过重用这部分缓冲区来提高性能。当在离屏缓冲区中的图层没有改变的时候，它就能够被重用。

需要注意的是，光珊化的图层的存储空间是有限的。苹果提示存储光珊化/离屏缓冲区有将近屏幕两倍大小的空间。如果你使用的方法会造成离屏渲染的话，你可能应该更好的尝试去除掉离屏渲染。使用蒙板或者给图层设置圆角，都会造成离屏渲染，应用阴影也一样。

至于带有圆角和设置了clipsBounds与maskToBounds属性的蒙板来说，你可能可以使用一张设置好的图片来当作对应的蒙板。当然这是一种平衡的方案。如果你想要在图层上应用矩形蒙板的话，你可能可以使用contentsRect属性而不是蒙板。

如果你已经设置shouldRasterize等于YES，记住设置rasterizationScale的值为contentsScale。

### 更多相关合成的信息

一如既往的，Wikipadia有更多关于透明度合成的数学背景。当讨论像素的时候， 我们将会更加深入的了解红，绿，蓝以及透明度是如何在内存中进行表示的。

### OS X

如果你正在OS X系统上工作的话，你会发现那些调试的选项叫做Quartz Debug，并且不在Instruments中。Quartz Debug是“Graphics Tools”的一部分，它的[下载地址](https://developer.apple.com/download/all/)。

### Core Animation & OpenGL ES

就像名字所建议的那样，Core Animation让你能够在屏幕上执行动画。我们将大部分时间跳过谈论动画，而专注于绘制。有件事需要说明一下，Core Animation允许你极有效率的渲染图像。这也是为什么你可以以每秒60帧去制作动画。

Core Animation的核心是OpenGL ES之上的抽象。简而言之，就是他可以让你使用OpenGL ES的强大能力，而不需要处理他的复杂性。当我们讨论上面的合成的时候，交替使用了图层和纹理的术语。他们不是一个东西，但是非常的相似。

Core Animation的图层可以有子图层，所以你最中得到的是一个图层树。Core Animation所需要做的繁重的工作就是确定需要绘制（重绘）那些图层，并且调用那些OpenGL ES去合成这些图层，然后展示到屏幕上去。

举个例子，当你将图层内容赋值给CGImageRef的时候，Core Animation会创建一个Oopen GL ES的纹理，并保证这个图片中的位图会被上传到相应的纹理。或者如果你重写了-drawinContext方法，Core Animation会分配一个纹理，并且保证你调用的Core Graphics会被转变为改纹理的位图数据。图层的属性和CALayer的子类影响了OpenGL是如何实现渲染的，很多的低级的OpenGL ES的行为都很好的被封装成了易于理解的CALayer的概念。

Core Animation通过Core Graphics与OpenGL ES编排了基于CPU的位图绘制。正是因为Core Animation处在渲染管道至关重要的位置，如何使用Core Animation就会显著的影响性能。

### CPU限制 vs GPU限制

当你想要在屏幕上展示一些东西的时候，这就会有很多组件在一起协作完成。其中最重要的组件就是CPU和GPU。它们名字中的P和U表示处理单元（progressing unit），当要在屏幕上绘制东西的时候，它们将会对这些进行处理。CPU与GPU都有有限的资源。

为了达到每秒60帧，你必须保证CPU和GPU都没有超负荷工作。在此基础之上，你想要让GPU承担尽可能多的工作。你想要让CPU被释放去执行更多的应用代码而不是忙于执行绘制工作。很多时候，GPU比CPU更擅长进行渲染，并且会降低系统整体的负载以及能耗。

既然绘制性能取决于CPU跟GPU，你需要去弄清楚哪个限制了你的绘制性能。如果你使用了所有的GPU资源，那么GPU就是限制了你的性能，你在绘制的时候就会说GPU限制。同样的，如果你最大限度的使用了CPU，那么你就会说CPU限制。

如果你是CPU限制，你需要降低GPU的负载（或许是让CPU做更多的工作）。如果你是CPU限制，你就需要降低CPU的负载。

如果你是GPU限制，那么使用OpenGL ES驱动工具。点击i按钮，然后配置它，确保设备的利用率%是被选中的。现在，当你运行你的App的时候，你会看到GPU是怎么加载的。如果这个数字接近100%，说明你在GPU上尝试做了很多工作。

受CPU限制是你的app做了很多传统方面的工作。Time Profile工具回帮助你排查。

### Core Graphics / Quartz 2D

Quartz 2D更为我们熟知的是包含它的框架的名字：Core Graphics。

Quantz 2D有更多的技巧，我们不能在这里介绍。我们不准备去谈论关于PDF创建，渲染，解析，输出这个庞大的模块。请注意，打印和PDF创建的过程与把位图绘制到屏幕上是非常相似的，因为他们都是基于了Quarts 2D。

让我们简要的谈谈Quartz 2D的相关概念。如果想要了解更多信息，请到Apple的官方文档[Quartz 2D Programming Guide](https://developer.apple.com/library/archive/#documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/Introduction/Introduction.html)。

放心，Quarts 2D在2D图形的绘制上非常的强大。举几个例子，如基于路径的绘制，抗锯齿的渲染，透明层，分辨率以及设备无关性等。它可能看着非常让人畏惧，因为相比于其他的，它更加的底层以及基于C的API。

不过主要的概念相对来说比较简单。UIKit与AppKit都封装了能被简单实用的Quarts 2D API，一旦你习惯使用它之后，你甚至可以去访问一些普通的C API。你最终会得到一个绘制引擎，它可以做绝大多数PhotoShop与Illustrator能做的。Apple提到了iOS上的股票应用是Quarts 2D的一个简单实力，因为该图是使用Quarts 2D在代码中动态渲染的一个简单示例。

当你的应用要绘制位图时，那么它或多或少会基于Quarts 2D。也就是说，绘图的CPU部分将会被Quarts 2D执行。虽然Quarts可以做一些其他的事情，但是这里我们只专注于位图的绘制，比如说在包含RGBA数据的缓冲区中绘制结果。

假设我们要绘制一个八角形。我们可以使用UIKit进行绘制。

```objective-c
UIBezierPath *path = [UIBezierPath bezierPath];
[path moveToPoint:CGPointMake(16.72, 7.22)];
[path addLineToPoint:CGPointMake(3.29, 20.83)];
[path addLineToPoint:CGPointMake(0.4, 18.05)];
[path addLineToPoint:CGPointMake(18.8, -0.47)];
[path addLineToPoint:CGPointMake(37.21, 18.05)];
[path addLineToPoint:CGPointMake(34.31, 20.83)];
[path addLineToPoint:CGPointMake(20.88, 7.22)];
[path addLineToPoint:CGPointMake(20.88, 42.18)];
[path addLineToPoint:CGPointMake(16.72, 42.18)];
[path addLineToPoint:CGPointMake(16.72, 7.22)];
[path closePath];
path.lineWidth = 1;
[[UIColor redColor] setStroke];
[path stroke];
```

这或多或少对应如下的Core Graphics的代码：

```objective-c
CGContextBeginPath(ctx);
CGContextMoveToPoint(ctx, 16.72, 7.22);
CGContextAddLineToPoint(ctx, 3.29, 20.83);
CGContextAddLineToPoint(ctx, 0.4, 18.05);
CGContextAddLineToPoint(ctx, 18.8, -0.47);
CGContextAddLineToPoint(ctx, 37.21, 18.05);
CGContextAddLineToPoint(ctx, 34.31, 20.83);
CGContextAddLineToPoint(ctx, 20.88, 7.22);
CGContextAddLineToPoint(ctx, 20.88, 42.18);
CGContextAddLineToPoint(ctx, 16.72, 42.18);
CGContextAddLineToPoint(ctx, 16.72, 7.22);
CGContextClosePath(ctx);
CGContextSetLineWidth(ctx, 1);
CGContextSetStrokeColorWithColor(ctx, [UIColor redColor].CGColor);
CGContextStrokePath(ctx);
```

我们需要知道这个它会被绘制到那里。这也是CGContext发挥作用的地方。我们传递的ctx参数就在这个上下文中。并且上下文定义了我们将绘制到哪里去。如果我们实现了CALayer的-drawInContext的方法：我们正在传递上下文。在这个上下文中进行绘制，结果将被绘制到layer的backing store（也就是它的缓冲区）中去。但是我们也可以创建自己的上下文，即位图相关的上下文，比如CGBitmapContextCreate()。这个函数会返回一个上下文，然后我们可以传递给CGContext函数以绘制到该上下文。

注意UIKit版本的代码是不用在方法中传递上下文的。因为当使用UIKit或者AppKit的时候，上下文是隐式的。UIKit拥有一个上下文的栈，并且UIKit方法总是把内容绘制到栈顶部的上下文中去。你可以使用UIGraphicsGetCurrentContext()方法获取到当前的上下文。你可以使用UIGraphicsPushContext()和UIGraphicsPopContext()方法在栈中推入或者弹出上下文。

最值得注意的是，UIKit有一个方便的方法UIGraphicsBeginImageContextWithOptions()和UIGraphicsEndImageContext()去创建位图上下文，类似于CGBitmapContextCreate()。混合使用UIKit和Core Graphics调用相当简单：

```objective-c
UIGraphicsBeginImageContextWithOptions(CGSizeMake(45, 45), YES, 2);
CGContextRef ctx = UIGraphicsGetCurrentContext();
CGContextBeginPath(ctx);
CGContextMoveToPoint(ctx, 16.72, 7.22);
CGContextAddLineToPoint(ctx, 3.29, 20.83);
...
CGContextStrokePath(ctx);
UIGraphicsEndImageContext();
```

或者反过来：

```objective-c
CGContextRef ctx = CGBitmapContextCreate(NULL, 90, 90, 8, 90 * 4, space, bitmapInfo);
CGContextScaleCTM(ctx, 0.5, 0.5);
UIGraphicsPushContext(ctx);
UIBezierPath *path = [UIBezierPath bezierPath];
[path moveToPoint:CGPointMake(16.72, 7.22)];
[path addLineToPoint:CGPointMake(3.29, 20.83)];
...
[path stroke];
UIGraphicsPopContext(ctx);
CGContextRelease(ctx);
```

我们可以用Core Graphics做很多很酷的事情。有个很好的理由是，苹果的文档提供了很多输出的保真度。我们不能够获取到所有细节，但是Core Graphics有一个图像模型，它的工作原理非常接近于Adobe Illustrator和Adobe Photoshop。大部分的工具的概念都会转化为Core Graphics。毕竟它起源于使用Display PostScript的NeXTStep。

### CGLayer

我们最初表示可以通过CGLayer加速同一个元素的重复绘制。正如Dave hayden所说的那样，街上的话不再为真（太哲学了吧）。

### 像素

屏幕上的像素由红，绿，蓝三种颜色组成。因此位图数据有时也被称为RGB数据。你可能想要知道，这个数据在内存中是怎么组织的。但是事实是，有很多很多的RGB位图在内存中的表示方式。

稍后我们会讨论数据压缩，这又是完全不同的。现在，我们来看看RGB位图数据，每一个颜色分量都有一个值：红，绿，蓝。并且更经常的，我们还会有第四个分量的值：透明度。每个像素都由这四个独立的值组成。

### 默认的像素布局方式

在iOS和OS X中，最常见的格式有每个像素32位（bpp），每个颜色部分8位（bpc），并且都先预乘透明度。在内存中，就像下面这样：

```
A   R   G   B   A   R   G   B   A   R   G   B  
| pixel 0       | pixel 1       | pixel 2   
  0   1   2   3   4   5   6   7   8   9   10  11 ...
```

这种格式通常被叫做ARGB。每个像素占用四个字节。每个颜色部分占用一个字节。每个像素都有透明度，透明度展示在最前面。最终红绿蓝三种颜色都会预乘透明度。预乘的意思就是说，每个颜色都乘以透明度。如果我们有个橘色的像素，那么它的RGB的值分别是255，240， 99， 24。如果这个像素是完全不透明的，那么它的ARGB在内存中就如上面所说是255，240， 99， 24。但是如果这个像素的透明度是33%，那么像素值就为84，80， 33， 8。

另一种通用的格式是32bpp，8bpc，没有透明度，就如下所示：

```
 x   R   G   B   x   R   G   B   x   R   G   B  
| pixel 0       | pixel 1       | pixel 2   
  0   1   2   3   4   5   6   7   8   9   10  11 ...
```

 这也叫做xRGB。这个像素没有任何的透明度值（他们被假设透明度为100%），但是在内存布局上是相同的。你可能想为什么这种格式这么流行，甚至于如果不在内存中使用透明度的值，那么内存空间可以节省25%。但是我们之所以还是采用这种内存方式，是因为对于现代的CPU和图像算法来说，这样计算会更加容易一些，也是因为这样可以做到内存对齐。现代CPU不喜欢加载没有对齐的内存。一些算法也做很多移动和蒙板的工作，特别是当需要一些混合那些有透明度的格式。

当处理RGB数据的时候，Core Graphics也支持放置透明度值到最后。这有时候会被叫做RGBA和RGBx，隐式的假设每个颜色占用8位，并且都预乘了透明度。

### 图像格式

你在iOS或者OS X中处理的硬盘中的图片，大部分都是JPEG和PNG格式的图片。让我们更加仔细的看看吧。

### JPEG

每个人都知道JPEG。它来自于相机。这就是计算机上存储照片的方式，甚至你妈妈也听说过JPEG。很多人都觉的JPEG文件的内存存储方式就跟上述一样是，使用RGB布局。这和现实差的非常远。

把JPEG的数据转变为像素是非常复杂的过程，你肯定需要花费非常的时间才能了解它。对于每个颜色区域，JPEG压缩使用了基于离散余弦变化将空间信息转化为频域（那么，什么事频域呢。）这个信息然后通过各种哈夫曼编码进行量化，序列化以及打包。然后经常性的，数据会被从RGB转化为YCbCr。当解码一张JPEG图片的时候，所有的操作都需要被反向执行一遍。

这也是为什么当你创建一张JPEG文件的UIImage对象的时候，并将它绘制到屏幕上是，它会产生延时，因为CPU在解码的时候非常的忙碌。如果你必须在tableview cell中解码JPEG文件的时候，你的滑动将不会顺畅。

所以为什么我们需要使用JPEG图片呢？答案就是JPEG可以将图片压缩的非常非常好。一张在你iPhone5中没有被压缩的图片，会占用你24MB的内存。使用默认的压缩设置，你相机中的图片通常只有2MB-3MB。JPEG压缩工作的非常好的原因就是，它是有损压缩的。它会抛弃一些人眼无法察觉的信息，这样做可以将远远超过一些普通算法，例如gzip能做的。但是这个只在图片中工作的非常好，因为JPEG是基于一个事实，那就是照片中存在很多人类不易察觉的信息。如果你给一个几乎全是文字的网页截屏的话，那么JPEG就不会执行的非常好。压缩将会更低，然后你将会看到JPEG图片真的改变了图片。（JPEG有一个质量值，默认可以设置为0.9）。

### PNG

PNG的发音为“Ping”。和JPEG相反，它是一种无损的压缩方法。当你保存一张图片为PNG格式，那么当你不久打开它之后，所有的像素值都和你最初的一样。因为这种限制，PNG压缩的图片不能和JPEG一样好，但是对于app组件比如按钮，图标等，它工作的非常好。还有就是，PNG数据的解压方式比JPEG要简单很多。

在现实生活中，事情从不会这样简单，它有一堆不同的PNG格式。你们可以去维基百科了解更多细节。但是简单的说，PNG支持有无alpha通道的像素压缩。这也是为什么PNG适合应用程序。

### 选择格式

当你在应用中使用照片的时候，你应该坚持使用这两个中的一个：JPEG或者PNG。解码器跟压缩器能够非常高效的对这些格式进行读写，对于一些扩展，支持并行操作。

